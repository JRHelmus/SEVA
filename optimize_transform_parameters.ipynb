{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import pandas\n",
    "import pickle\n",
    "import numpy\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import scipy\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_error(expected_pattern, simulated_pattern):\n",
    "        ''' This method calculates the error between two activity patterns.\n",
    "            Each bin in the expected activity pattern is compared to the\n",
    "            simulated activity pattern and the error is calculated. The method\n",
    "            returns the mean error over all the bins in the patterns. Two types\n",
    "            of error calculation are supported: the Mean Abolute Error (MEA)\n",
    "            and the relative MAE. The MAE of a bin is defined as the difference\n",
    "            between the simulated and real value.  The relative error of a bin\n",
    "            is defined as the difference between the simulated and real value\n",
    "            the bin divided by the maximum error of this bin. The maximum error\n",
    "            is the maximum distance of the real bin to either 0 or 1. The error\n",
    "            is then (in both cases) multiplied by 100 to get the error range\n",
    "            between 0 and 100.\n",
    "\n",
    "        Args:\n",
    "            expected_pattern (List[float]): The expected activity pattern.\n",
    "            simulated_pattern (List[float]): The simulated activity pattern.\n",
    "\n",
    "        Kwargs:\n",
    "            method (str): Method of validation. 'MAE' for Mean Absolute Error\n",
    "                or 'relMAE' for relative Mean Absolute Error. Default 'relMAE'.\n",
    "\n",
    "        Returns:\n",
    "            (float): The error between the expected and the simulated pattern.\n",
    "        '''\n",
    "\n",
    "        nr_bins = len(expected_pattern)\n",
    "        error = 0\n",
    "        for i in range(nr_bins):\n",
    "            expected = expected_pattern[i]\n",
    "            simulated = simulated_pattern[i]\n",
    "            difference = numpy.abs(simulated - expected)\n",
    "            max_distance = max(1 - expected, expected)\n",
    "            error += 100 * ( difference / max_distance)\n",
    "\n",
    "\n",
    "        return error / nr_bins\n",
    "\n",
    "def transform_dist_version_x(original_dist, transforming_dist, mean_target, mean_origin, difference_factors):\n",
    "    dist = [max(0, transform_val_version_x(original_dist[i], mean_target[i], mean_origin[i], transforming_dist[i],\n",
    "                                           difference_factors))\n",
    "            for i in range(len(original_dist))]\n",
    "    return dist / sum(dist)\n",
    "\n",
    "\n",
    "def transform_val_version_x(original_value, mean_target_value, mean_origin_value, difference_value, f):\n",
    "    dev_origin = original_value - mean_origin_value\n",
    "    dev_target = mean_target_value - original_value\n",
    "    return original_value + f[0] * difference_value + f[1] * dev_origin + f[2] * dev_target\n",
    "\n",
    "def get_optimal_transform_factor_version_x(original_dist, transform_dist, target_dist):\n",
    "    best_score = 0.0\n",
    "    best_combination = [0, 0, 0]\n",
    "    history_scores = []\n",
    "    history_best_cobinations = []\n",
    "    size = len(transform_dist)\n",
    "    max_factor = MAX_FACTOR\n",
    "    min_factor = MIN_FACTOR\n",
    "    step_size = STEP_SIZE\n",
    "    temp = datetime.datetime.now()\n",
    "    agents = list(data_agents)[:NUMBER_OF_AGENTS]\n",
    "    dc_agents = {}\n",
    "    score_counter = 0\n",
    "    for agent in agents:\n",
    "        dc_agent = sum_distributions(data_agents[agent]['disconnection_duration_dists'])\n",
    "        dc_agent = dc_agent[:size] / sum(dc_agent[:size])\n",
    "        dc_agents[agent] = list(dc_agent)\n",
    "    for f0 in numpy.arange(min_factor, max_factor, step_size):\n",
    "        print(f0, datetime.datetime.now() - temp)\n",
    "        temp = datetime.datetime.now()\n",
    "        for f1 in numpy.arange(min_factor, max_factor, step_size):\n",
    "            for f2 in numpy.arange(min_factor, max_factor, step_size):\n",
    "                factors_score = []\n",
    "                for agent in agents:\n",
    "                    pred = transform_dist_version_x(dc_agents[agent][:size], transform_dist[:size],\n",
    "                                target_dist[:size], original_dist[:size], difference_factors = [f0, f1, f2])\n",
    "                    naive_error = get_error(dc_agents[agent][:size], target_dist[:size])\n",
    "                    target_error = get_error(pred, target_dist[:size])\n",
    "                    score = naive_error - target_error\n",
    "                    factors_score.append(score)\n",
    "                score = numpy.mean(factors_score)\n",
    "                if (score > best_score):\n",
    "                    score_counter +=1\n",
    "                    if score_counter > 10:\n",
    "                        print('new best score (%.3f) for parameters %s' %(score, [f0, f1, f2]))\n",
    "                    best_score = score\n",
    "                    best_combination = [f0, f1, f2]\n",
    "                if score > 0.01:\n",
    "                    history_scores.append(score)\n",
    "                    history_best_cobinations.append([f0, f1, f2])\n",
    "\n",
    "    return best_combination, best_score, history_best_cobinations, history_scores\n",
    "\n",
    "def sum_timedeltas(x):\n",
    "    sum_ = datetime.timedelta(minutes = 0)\n",
    "    for item in x:\n",
    "        sum_ = operator.add(item, sum_)\n",
    "    return sum_\n",
    "\n",
    "def sum_distributions(distributions):\n",
    "    total_distribution = pandas.Series()\n",
    "    count = 0\n",
    "    for dist in distributions:\n",
    "        if isinstance(dist, pandas.DataFrame):\n",
    "            continue\n",
    "        if total_distribution.empty:\n",
    "            count +=1\n",
    "            total_distribution = dist\n",
    "        else:\n",
    "            count+=1\n",
    "            total_distribution = total_distribution.radd(dist, level=None, fill_value=0, axis=0)\n",
    "    return total_distribution.astype(int)\n",
    "\n",
    "def load_agent(directory, agent_ID):\n",
    "    with open(directory + agent_ID + '.pkl', 'rb') as agent_file:\n",
    "        agent_data = pickle.load(agent_file)\n",
    "    return agent_data\n",
    "\n",
    "def load_dict_agents_data(directory, skip_changing_users = True, check_regular = True):\n",
    "    high_to_low_users = ['5D763B2DCBFD274859F43EE1456A1DD0B68F72A82E0A8564372C4BD46B95DAC8',\n",
    "                        '8DD214E9F9611E7D01E662E940AC973BCC34CF45C3D43D3306DFD3F9AEB5075C',\n",
    "                        '9A79FD1E5649B3FA3978C6B7E682571ED5570F5D1A35CDE4E80C6F30DFCFAF2B',\n",
    "                        '09CAA3715AE1BC35FC152487BBD693275E97DE039207824D1063E5A7A0E2DD6E',\n",
    "                        '144984E1475253AC45D743910C4CF344A2D315EA12ACD21AB58971105DEAF522',\n",
    "                        'B8256507B35A603D43CF7437CCC5712F33F1F54B82EB416FEA942725AAD3831C',\n",
    "                        'F32DBA25D91138963F85144783F71A16E4C2CF7D1523F0644A6404B4C2F227C4',\n",
    "                        'F9541CDC30F3E36CEE3EE6954D0AC7D7925C5539D5E751E79AD13734E455ADBC']\n",
    "    low_to_high_users = ['6D47DA110FAA422E77B8DA82975F644315AC05C3D0E38B12A80816793452D857',\n",
    "                        '8C45CDFC57F283C9BFB268CBD39725923FF8D22BCDF75E3E4FD054B079872459',\n",
    "                        '9C510710140B57F6EB58CC760CCAE70064D161D0B993741CC479FAF65175B2CB',\n",
    "                        '9D2D9AD61D9BCBF8A3CBBBEE6A031A7152EF2C28235E54CC5E1A478CC9BC9F73',\n",
    "                        '17AFFF335DB8BF72259DD11C23D375ECA607613B2D9E1379AFF0922888A8EE90',\n",
    "                        '53DDDDFF98D9CB22915B054A55F663BFDE941FFD8039A03248E559945C096031',\n",
    "                        '80E9884E3E4B152B66E2B3A2861FE89BE1D48A346C16EF8739F9714F4BD6AEBB',\n",
    "                        '606BD9C4F45EF4BD2710D2C4B178BF7356D2D9437BDF53B5593AB87F35A061B1',\n",
    "                        '17897EE886276B2EA379234AC9112C0816C0C917977A8DB4E3282A3C9BE7D018',\n",
    "                        '918130F5F1FC762D62308F082000F698F77C6CDBD34F9F4915835D484663A509',\n",
    "                        'EDA3B229C2661C6E479A565173F553DB7A10275D5360E542DA626F96832C12C6',\n",
    "                        'F346C8B51850F3D811CCDE0EC612153028C9F69E9B79C1FA9E4F5FA75F470AA9',\n",
    "                        'FDC8791D2EE934F8A1DD22816E149DD2E37E185C985C772D38D44FE74AA207DE',\n",
    "                        'F20D73D834CB2523C7E8E55643E2A7E6467DA5D4B4EBCA3547BC9FE49009E731']\n",
    "    wth_are_u_doing_users = ['7DA773D56D9BABA6B82139F286FA593EB0685317075EE743F05A3DDE1A1A552A',\n",
    "                          '62ABF94F3B61963B34B78EA7F062DE8681015F17730270CCAA8F6BD61828889F',\n",
    "                          '70E685BA7C2BC723DA307881476A3E5FC3AD1FCABDDFBF3A5BBA69E8BBB53B94']\n",
    "    result = {}\n",
    "    all_agent_IDs = set([files[:-4] for files in os.listdir(directory) if files[0] != '.'])\n",
    "    for ID in all_agent_IDs:\n",
    "        if skip_changing_users and (ID in high_to_low_users or ID in low_to_high_users):\n",
    "            continue\n",
    "        agent_data = load_agent(directory, ID)\n",
    "        if check_regular and agent_data['user_type'] != 'regulier':\n",
    "            continue\n",
    "        else:\n",
    "            result[ID] = agent_data\n",
    "    return result\n",
    "\n",
    "def print_and_save_results(best_factors, best_score, history_factors, history_scores, filename):\n",
    "    results = {}\n",
    "    results['best_factors'] = best_factors\n",
    "    results['best_score'] = best_score\n",
    "    results['history_factors'] = history_factors\n",
    "    results['history_scores'] = history_scores\n",
    "    resulting_file = result_directory + filename + '.pkl'\n",
    "    with open(resulting_file, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print('Results: factor = %s, score = %.5f' %(best_factors, best_score))\n",
    "    print('Results saved in %s' %resulting_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_AGENTS = 300\n",
    "MIN_FACTOR = -3\n",
    "MAX_FACTOR = 3\n",
    "STEP_SIZE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_agents = load_dict_agents_data('data/agent_database/all_non_changing_agents/')\n",
    "directory = 'data/simulation_pkls/'\n",
    "with open(directory + 'disconnection_distribution_phev.pkl', 'rb') as f:\n",
    "    disconnection_distribution_phev = pickle.load(f)\n",
    "with open(directory + 'disconnection_distribution_high_fev.pkl', 'rb') as f:\n",
    "    disconnection_distribution_high_fev = pickle.load(f)\n",
    "with open(directory + 'disconnection_distribution_low_fev.pkl', 'rb') as f:\n",
    "    disconnection_distribution_low_fev = pickle.load(f)\n",
    "with open(directory + 'connection_distribution_phev.pkl', 'rb') as f:\n",
    "    connection_distribution_phev = pickle.load(f)\n",
    "with open(directory + 'connection_distribution_high_fev.pkl', 'rb') as f:\n",
    "    connection_distribution_high_fev = pickle.load(f)\n",
    "with open(directory + 'connection_distribution_low_fev.pkl', 'rb') as f:\n",
    "    connection_distribution_low_fev = pickle.load(f)\n",
    "with open(directory + 'transformation_disconnection_phev_to_high.pkl', 'rb') as f:\n",
    "    diff_dc_phev_to_high = pickle.load(f)\n",
    "with open(directory + 'transformation_disconnection_phev_to_low.pkl', 'rb') as f:\n",
    "    diff_dc_phev_to_low = pickle.load(f)\n",
    "with open(directory + 'transformation_connection_phev_to_high.pkl', 'rb') as f:\n",
    "    diff_con_phev_to_high = pickle.load(f)\n",
    "with open(directory + 'transformation_connection_phev_to_low.pkl', 'rb') as f:\n",
    "    diff_con_phev_to_low = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_score(x, dists, size, original_dist, transform_dist, target_dist):\n",
    "    f0 = x[0]\n",
    "    f1 = x[1]\n",
    "    f2 = x[2]\n",
    "    factors_score = []\n",
    "    for dist in dists:\n",
    "        this_size = min(size, len(dist))\n",
    "        if agent_size == 0:\n",
    "            print('empty dist')\n",
    "            continue\n",
    "        pred = transform_dist_version_x(dist[:this_size], transform_dist[:this_size],\n",
    "                    target_dist[:agent_size], original_dist[:this_size], difference_factors = [f0, f1, f2])\n",
    "        naive_error = get_error(dist[:this_size], target_dist[:this_size])\n",
    "        target_error = get_error(pred, target_dist[:this_size])\n",
    "        score = naive_error - target_error\n",
    "        factors_score.append(score)\n",
    "    score = numpy.mean(factors_score)\n",
    "    return -1.0*score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dists_centers = []\n",
    "for agent in agents:\n",
    "    for center in data_agents[agent]['centers_info']:\n",
    "        dist = sum_distributions(data_agents[agent]['connection_duration_dists'][center])\n",
    "        dist = dist[:size] / sum(dist[:size])\n",
    "        dists_centers.append(list(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''diff_dc_phev_to_high \n",
    "[0.16259663, -0.16202726,  0.83797553]\n",
    "-0.15294582787757854\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('test.pkl', 'wb') as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting basin hopping\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 0: f -0.186435\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 1: f -0.186435 trial_f -0.186435 accepted 1  lowest_f -0.186435\n",
      "found new global minimum on step 1 with function value -0.186435\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 2: f -0.186435 trial_f -0.186435 accepted 1  lowest_f -0.186435\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 3: f -0.186435 trial_f -0.186435 accepted 1  lowest_f -0.186435\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 4: f -0.186435 trial_f -0.186435 accepted 1  lowest_f -0.186435\n",
      "adaptive stepsize: acceptance rate 0.800000 target 0.500000 new stepsize 0.555556 old stepsize 0.5\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 5: f -0.186435 trial_f -0.186435 accepted 1  lowest_f -0.186435\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 6: f -0.186435 trial_f -0.186435 accepted 1  lowest_f -0.186435\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 7: f -0.186435 trial_f -0.186435 accepted 1  lowest_f -0.186435\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 8: f -0.186435 trial_f -0.186435 accepted 1  lowest_f -0.186435\n",
      "found new global minimum on step 8 with function value -0.186435\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 9: f -0.186435 trial_f -0.186435 accepted 1  lowest_f -0.186435\n",
      "adaptive stepsize: acceptance rate 0.900000 target 0.500000 new stepsize 0.617284 old stepsize 0.555556\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 10: f -0.186435 trial_f -0.186435 accepted 1  lowest_f -0.186435\n",
      "0:08:46.442762\n",
      "                        fun: -0.18643502932999603\n",
      " lowest_optimization_result:       fun: -0.18643502932999603\n",
      " hess_inv: array([[ 0.03945079, -0.01009687, -0.01191235],\n",
      "       [-0.01009687,  0.00930546,  0.01097324],\n",
      "       [-0.01191235,  0.01097324,  0.01297148]])\n",
      "      jac: array([ -5.92321157e-06,  -1.03809871e-03,   1.34912692e-03])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 369\n",
      "      nit: 18\n",
      "     njev: 72\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([-0.95060062,  0.95061095,  1.95061097])\n",
      "                    message: ['requested number of basinhopping iterations completed successfully']\n",
      "      minimization_failures: 11\n",
      "                       nfev: 3322\n",
      "                        nit: 10\n",
      "                       njev: 642\n",
      "                          x: array([-0.95060062,  0.95061095,  1.95061097])\n"
     ]
    }
   ],
   "source": [
    "agents = set(list(data_agents)[:20])\n",
    "dc_agents = {}\n",
    "original_dist = connection_distribution_phev\n",
    "transform_dist = list(diff_con_phev_to_high)\n",
    "target_dist = connection_distribution_high_fev\n",
    "size = len(transform_dist)\n",
    "for agent in agents:\n",
    "    dc_agent = sum_distributions(data_agents[agent]['disconnection_duration_dists'])\n",
    "    dc_agent = dc_agent[:size] / sum(dc_agent[:size])\n",
    "    dc_agents[agent] = list(dc_agent)\n",
    "print(\"starting basin hopping\")\n",
    "start = datetime.datetime.now()\n",
    "result = scipy.optimize.basinhopping(get_score, [0,0,0],  niter=10, T=1.0, stepsize=0.5, \n",
    "    minimizer_kwargs = {\"args\": (agents, dc_agents, size, original_dist, transform_dist, target_dist)}, \n",
    "    take_step=None, accept_test=None, callback=None, interval=5, disp=True, niter_success=None)\n",
    "print(datetime.datetime.now() - start)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('getting optimal disconnection phev to high factors')\n",
    "result_directory = 'data/battery_size/'\n",
    "best_factors, best_score, history_factors, history_scores = get_optimal_transform_factor_version_x(disconnection_distribution_phev,\n",
    "    list(diff_dc_phev_to_high), disconnection_distribution_high_fev)\n",
    "print('disconnection phev to high')\n",
    "filename = 'phev_to_high_disconnection_transformation_factors_' + str(NUMBER_OF_AGENTS) + '_' + str(MIN_FACTOR) + '_' + str(MAX_FACTOR) + '_' + str(STEP_SIZE)\n",
    "print_and_save_results(best_factors, best_score, history_factors, history_scores, filename)\n",
    "\n",
    "# print('getting optimal disconnection phev to low factors')\n",
    "# best_factors, best_score, history_factors, history_scores = get_optimal_transform_factor_version_x(disconnection_distribution_phev,\n",
    "#     list(diff_dc_phev_to_low), disconnection_distribution_low_fev)\n",
    "# print('disconnection phev to low')\n",
    "# filename = 'phev_to_low_disconnection_transformation_factors_' + str(NUMBER_OF_AGENTS) + '_' + str(MIN_FACTOR) + '_' + str(MAX_FACTOR) + '_' + str(STEP_SIZE)\n",
    "# print_and_save_results(best_factors, best_score, history_factors, history_scores, filename)\n",
    "\n",
    "# print('getting optimal connection phev to high factors')\n",
    "# best_factors, best_score, history_factors, history_scores = get_optimal_transform_factor_version_x(connection_distribution_phev,\n",
    "#     list(diff_con_phev_to_high), connection_distribution_high_fev)\n",
    "# print('connection phev to high')\n",
    "# filename = 'phev_to_high_connection_transformation_factors_' + str(NUMBER_OF_AGENTS) + '_' + str(MIN_FACTOR) + '_' + str(MAX_FACTOR) + '_' + str(STEP_SIZE)\n",
    "# print_and_save_results(best_factors, best_score, history_factors, history_scores, filename)\n",
    "\n",
    "# print('getting optimal connection phev to low factors')\n",
    "# best_factors, best_score, history_factors, history_scores = get_optimal_transform_factor_version_x(connection_distribution_phev,\n",
    "#     list(diff_con_phev_to_low), connection_distribution_low_fev)\n",
    "# print('connection phev to low')\n",
    "# filename = 'phev_to_low_connection_transformation_factors_' + str(NUMBER_OF_AGENTS) + '_' + str(MIN_FACTOR) + '_' + str(MAX_FACTOR) + '_' + str(STEP_SIZE)\n",
    "# print_and_save_results(best_factors, best_score, history_factors, history_scores, filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
